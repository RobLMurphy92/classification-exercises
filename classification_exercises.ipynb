#!/usr/bin/env python
# coding: utf-8

# # Classification Exercises

# ## 1.) In a jupyter notebook, classification_exercises.ipynb, use a python module (pydata or seaborn datasets) containing datasets as a source from the iris data. Create a pandas dataframe, df_iris, from this data.

# In[4]:


# py import method

from pydataset import data

data('iris', show_doc=True)

df_iris = data('iris')


# In[5]:


df_iris.head(10)


# In[6]:


#seaborn method


# In[7]:


import seaborn as sns


# In[8]:


import numpy as np


# In[9]:


df_iris_seaborn_method = sns.load_dataset('iris')


# In[10]:


df_iris_seaborn_method.head(5)


# In[11]:


# a.)print the first 3 rows


# In[12]:


df_iris.iloc[:3]


# In[13]:


#b.) print the number of rows and columns (shape)


# In[14]:


df_iris.shape


# In[15]:


# c.) print the column names
for col in df_iris.columns:
    print(col)


# In[50]:


df_iris.columns


# In[51]:


df_iris.columns.to_list()


# In[16]:


#d. print the data type of each column
df_iris.info()


# In[52]:


df_iris.dtypes


# In[57]:


# e.)print the summary statistics for each of the numeric variables. 
# Would you recommend rescaling the data based on these statistics?

stat = df_iris.describe().T

# I would not rescale. All are in cm there is not a large range.


# In[58]:


stat['range'] = stat['max'] - stat['min']


# In[59]:


stat


# In[ ]:





# In[18]:


import pandas as pd


# ### 2.) Read the Table1_CustDetails table from the Excel_Exercises.xlsx file into a dataframe named df_excel.
# 
# 

# just note:
# 
# if you are using ravinders excel to delete the first row which contains empty columns can use below.
# 
# - churn_df.columns = churn_df.iloc[0]
# - churn_df = churn_df.drop(churn_df.index[0]

# In[60]:


# a.) assign the first 100 rows to a new dataframe, df_excel_sample
df_excel = pd.read_excel('df_excel.xlsx', sheet_name='Table1_CustDetails')
df_excel.head(10)

df_excel_sample = df_excel.head(100)


# In[62]:


df_excel_sample = df_excel.head(100)


# use pandas methods and attributes to do some initial summarization and exploration of your data.
# 
# - .head()
# 
# - .shape
# 
# - .info()
# 
# - .columns
# 
# - .dtypes
# 
# - .describe()
# 
# - .value_counts()

# In[61]:


df_excel.info()


# In[63]:


# b.) print the number of rows of your original dataframe
df_excel.shape[0]


# In[21]:


# c.) print the first 5 column names
df_excel.columns[:5]


# In[22]:


df_excel.info()


# In[65]:


# d.) print the column names that have a data type of object
object_column = df_excel.select_dtypes(include='object').columns.to_list()
object_column


# In[24]:


# another method to get columns which are data types is df_excel.where


# In[66]:


# e.) compute the range for each of the numeric variables.
numeric_variables = df_excel.select_dtypes(include=['float64', 'int64'] ).columns.to_list()
numeric_variables


# In[26]:


numeric_variables = df_excel.select_dtypes(include=[np.number] ).columns
numeric_variables


# In[27]:



numeric_v = df_excel.select_dtypes(include=[np.number])

numeric_v_range = numeric_v.max() - numeric_v.min()
numeric_v_range


# ### 3.) Read the data from this google sheet into a dataframe, df_google
# 
# 

# In[29]:


sheet_url = 'https://docs.google.com/spreadsheets/d/1Uhtml8KY19LILuZsrDtlsHHDC9wuDGUSe8LTEwvdI5g/edit#gid=341089357'
csv_export_url = sheet_url.replace('/edit#gid=', '/export?format=csv&gid=')
train_googlesheet = pd.read_csv(csv_export_url)
train_googlesheet.head(2)


# In[74]:


# a.) print the first 3 rows
train_googlesheet.iloc[:3]


# In[76]:


# b.) print the number of rows and columns
train_googlesheet.shape


# In[80]:


# c.) print the column names
train_googlesheet.columns


# In[31]:


# d.) print the data type of each column
train_googlesheet.dtypes


# In[33]:


# e.) print the summary statistics for each of the numeric variables
train_googlesheet.select_dtypes(include = ['int64', 'float64']).describe()


# In[46]:


# f.) print the unique values for each of your categorical variables
unique_cat = train_googlesheet.nunique()


# In[80]:


for col in train_googlesheet.columns:
    if train_googlesheet[col].dtypes == 'object':
        print(f'{col} has {train_googlesheet[col].nunique()} unique values.')


# ### Make a new python module, acquire.py to hold the following data aquisition functions:

# In[ ]:


#1.) Make a function named get_titanic_data that returns the titanic data from the codeup data science database as a pandas data frame. 
#Obtain your data from the Codeup Data Science Database.

import pandas as pd
import numpy as np
import os
from env import host, user, password

###################### Acquire Titanic Data ######################

def get_connection(db, user=user, host=host, password=password):
    '''
    This function uses my info from my env file to
    create a connection url to access the Codeup db.
    '''
    return f'mysql+pymysql://{user}:{password}@{host}/{db}'
    
    



# In[81]:


def new_titanic_data():
    '''
    This function reads the titanic data from the Codeup db into a df,
    write it to a csv file, and returns the df.
    '''
    # Create SQL query.
    sql_query = 'SELECT * FROM passengers'
    
    # Read in DataFrame from Codeup db.
    df = pd.read_sql(sql_query, get_connection('titanic_db'))
    
    return df


# In[ ]:


#2.) Make a function named get_iris_data that returns the data from the iris_db on the codeup data science database as a pandas data frame. 
#The returned data frame should include the actual name of the species in addition to the species_ids. 
#Obtain your data from the Codeup Data Science Database.

###################### Acquire Iris Data ######################

def new_iris_data():
    '''
    This function reads the iris data from the Codeup db into a df.
    '''
    sql_query = """
                SELECT species_id,
                species_name,
                sepal_length,
                sepal_width,
                petal_length,
                petal_width
                FROM measurements
                JOIN species
                USING(species_id)
                """
    
    # Read in DataFrame from Codeup db.
    df = pd.read_sql(sql_query, get_connection('iris_db'))
    
    return df



# In[ ]:


#3.) Once you've got your get_titanic_data and get_iris_data functions written, now it's time to add caching to them. 
# To do this, edit the beginning of the function to check for a local filename like titanic.csv or iris.csv. 
# If they exist, use the .csv file. If the file doesn't exist, then produce the SQL and pandas necessary to create a dataframe, then write the dataframe to a .csv file with the appropriate name.

def get_iris_data(cached=False):
    '''
    This function reads in iris data from Codeup database and writes data to
    a csv file if cached == False or if cached == True reads in iris df from
    a csv file, returns df.
    '''
    if cached == False or os.path.isfile('iris_df.csv') == False:
        
        # Read fresh data from db into a DataFrame
        df = new_iris_data()
        
        # Cache data
        df.to_csv('iris_df.csv')
        
    else:
        
        # If csv file exists or cached == True, read in data from csv file.
        df = pd.read_csv('iris_df.csv', index_col=0)
        
    return df



# In[82]:


def get_titanic_data(cached=False):
    '''
    This function reads in titanic data from Codeup database and writes data to
    a csv file if cached == False or if cached == True reads in titanic df from
    a csv file, returns df.
    '''
    if cached == False or os.path.isfile('titanic_df.csv') == False:
        
        # Read fresh data from db into a DataFrame.
        df = new_titanic_data()
        
        # Write DataFrame to a csv file.
        df.to_csv('titanic_df.csv')
        
    else:
        
        # If csv file exists or cached == True, read in data from csv file.
        df = pd.read_csv('titanic_df.csv', index_col=0)
        
    return df

